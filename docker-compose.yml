version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    pull_policy: if_not_present
    command: ["serve"]
    tty: true
    restart: always
    volumes:
      - ./Models:/root/.ollama/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  chunking:
    container_name: chunking
    build: .
    volumes:
      - ./know_how:/app/know_how
      - ./Models:/app/Models
      - ./db:/app/db
      - .:/app
    depends_on:
      - ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - TABLE_NAME=my_table
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: ["sleep", "infinity"]

  rag_api:  
    container_name: rag_api
    build: .  
    volumes:
      - ./db:/app/db  
      - .:/app      
    depends_on:
      - ollama
    restart: always
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - TABLE_NAME=my_table
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: ["uvicorn", "rag_api:app", "--host", "0.0.0.0", "--port", "8000"]
  
  open-webui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "3000:8080"
    volumes:
      - ./open-webui-data:/app/backend/data
    restart: always
    environment:
      - OLLAMA_BASE_URL=http://rag_api:8000
    depends_on:
      - ollama
      - rag_api